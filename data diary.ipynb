{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Diary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. step one was to watch the tutorial\n",
    "https://www.youtube.com/watch?v=QNKxzkNpsko\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then i started to scrape. My goal is to access the website, download the xlsx file from it.\n",
    "Several challenges I face here: (a) I was blocked by the website at first (b) I need to figure out how to locate the xlsx file and dowanload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#After deciding to use beautiful soup and requests to scrape the website. I tried:\n",
    "\n",
    "my_url = \"https://www.trade.gov.tw/Pages/Detail.aspx?nodeID=1375&pid=790878&dl_DateRange=all&txt_SD=&txt_ED=&txt_Keyword=&pageindex=1&history=\"\n",
    "raw_html = requests.get(my_url, headers=headers)\n",
    "\n",
    "#I was blocked by the website so I added headers\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "#and it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#after checking the website html i use the a tag that starts with /App_Ashx/File.ashx?FileID= to locate the xlsx download link. and then use urljoin function to join it with the base link to download \n",
    "\n",
    "base_url = \"https://www.trade.gov.tw\"\n",
    "\n",
    "file_tag = soup_doc.find('a', href=re.compile(r\"^/App_Ashx/File.ashx\\?FileID=\"))\n",
    "file_href = file_tag['href']\n",
    "\n",
    "full_url = urljoin(base_url, file_href)\n",
    "print(f\"Full file download link: {full_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to clean the data downloaded from the International Trade Administration of Taiwan, which presents several challenges:\n",
    "\n",
    "(a) The header is incorrect\n",
    "(b) The data is in Mandarin\n",
    "(c) The years are not in Common Era format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#so i make row 2 into my header\n",
    "df.columns = df.iloc[1]\n",
    "\n",
    "#then i drop the rows that are not relevant\n",
    "df = df.drop([0, 1, 2, df.index[-3], df.index[-2], df.index[-1]]).reset_index(drop=True)\n",
    "\n",
    "#and the ones are NaN (those are the empty rows saved for future months)\n",
    "df = df[~df.iloc[:, 1:].isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#i translated the column names into English\n",
    "df.columns = ['Month/Year', 'Total', 'Export', 'Import', 'Balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#I'm setting a function to transform ROC years into Common Era format. First I need to get rid of the \"年\" (Mandarin word for \"year\") and turn it into integer and then +1911 (the difference between Common Era format and ROC date format). I'm also replacing the Mandarin months with English names.\n",
    "\n",
    "def transform_date(value):\n",
    "    value = str(value)\n",
    "    if '年' in value:\n",
    "        year = int(value.replace('年', '').strip())\n",
    "        return 1911 + year\n",
    "    elif '月' in value:\n",
    "        month_map = {\n",
    "            '1月': 'Jan.', '2月': 'Feb.', '3月': 'Mar.', '4月': 'Apr.',\n",
    "            '5月': 'May', '6月': 'Jun.', '7月': 'Jul.', '8月': 'Aug.',\n",
    "            '9月': 'Sep.', '10月': 'Oct.', '11月': 'Nov.', '12月': 'Dec.'\n",
    "        }\n",
    "        return month_map.get(value, value)\n",
    "    return value\n",
    "\n",
    "df.loc[:, 'Month/Year'] = df['Month/Year'].apply(transform_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then i upload everything to github\n",
    "set up github action, set the yml to scrape the website once a month\n",
    "set up the live website https://ashley-yihui-lee.github.io/us-taiwan-trade/\n",
    "modify the html and added the graph from datawrapper\n",
    "\n",
    "The only challenge I encountered was that the YAML and GitHub Action didn’t automatically update as shown in the tutorial. After speaking with the TA, I learned that the issue was because the site I scrape only updates once a month, unlike the real-time AQI site in the tutorial. Since there were no changes, nothing updated on GitHub.\n",
    "\n",
    "and side note: I noticed the example website https://laurabejder.com/european_energy_prices/ has a lighting icon, so I modified my HTML to add a Taiwan icon! That’s my favorite part of the project."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
